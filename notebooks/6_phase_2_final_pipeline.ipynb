{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aeJaQy29HqxB",
        "outputId": "bc9056a8-8d53-4260-8dcb-4918f71651a1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<>:43: SyntaxWarning: invalid escape sequence '\\$'\n",
            "<>:43: SyntaxWarning: invalid escape sequence '\\$'\n",
            "/tmp/ipython-input-3469248937.py:43: SyntaxWarning: invalid escape sequence '\\$'\n",
            "  df[col] = df[col].replace({'\\$': '', ',': ''}, regex=True)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading and cleaning raw data...\n",
            "Engineering new features...\n",
            "Preparing data for the final model...\n",
            "Training the final V2 pipeline on all data...\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002640 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 385\n",
            "[LightGBM] [Info] Number of data points in the train set: 146423, number of used features: 58\n",
            "[LightGBM] [Info] Start training from score 9.560788\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
            "Training complete.\n",
            "\n",
            "✅ V2 pipeline saved successfully as 'insurance_pipeline_v2.joblib'!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from lightgbm import LGBMRegressor\n",
        "import joblib\n",
        "\n",
        "# =============================================================================\n",
        "# 1. Data Loading and Cleaning\n",
        "# This section reproduces the successful cleaning steps from your notebook.\n",
        "# =============================================================================\n",
        "print(\"Loading and cleaning raw data...\")\n",
        "\n",
        "# Load the raw data\n",
        "# Make sure the path to your raw data is correct\n",
        "df = pd.read_csv('/content/Medicare_IP_Hospitals_by_Provider_and_Service_2023.csv', encoding='latin1')\n",
        "\n",
        "# Rename columns for clarity\n",
        "df = df.rename(columns={\n",
        "    \"ï»¿Rndrng_Prvdr_CCN\": \"provider_id\",\n",
        "    \"Rndrng_Prvdr_Org_Name\": \"provider_name\",\n",
        "    \"Rndrng_Prvdr_City\": \"city\",\n",
        "    \"Rndrng_Prvdr_St\": \"state\",\n",
        "    \"Rndrng_Prvdr_State_FIPS\": \"state_fips\",\n",
        "    \"Rndrng_Prvdr_Zip5\": \"zip_code\",\n",
        "    \"Rndrng_Prvdr_State_Abrvtn\": \"state_abbr\",\n",
        "    \"Rndrng_Prvdr_RUCA\": \"ruca_code\",\n",
        "    \"Rndrng_Prvdr_RUCA_Desc\": \"ruca_desc\",\n",
        "    \"DRG_Cd\": \"drg_code\",\n",
        "    \"DRG_Desc\": \"drg_desc\",\n",
        "    \"Tot_Dschrgs\": \"total_discharges\",\n",
        "    \"Avg_Submtd_Cvrd_Chrg\": \"avg_submitted_charge\",\n",
        "    \"Avg_Tot_Pymt_Amt\": \"avg_total_payment\",\n",
        "    \"Avg_Mdcr_Pymt_Amt\": \"avg_medicare_payment\"\n",
        "})\n",
        "\n",
        "# Correct data types for money and discharge columns\n",
        "numeric_columns = ['avg_submitted_charge', 'avg_total_payment', 'avg_medicare_payment', 'total_discharges']\n",
        "for col in numeric_columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        df[col] = df[col].replace({'\\$': '', ',': ''}, regex=True)\n",
        "    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "# Drop rows with missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# =============================================================================\n",
        "# 2. Feature Engineering\n",
        "# This section reproduces your successful feature engineering steps.\n",
        "# =============================================================================\n",
        "print(\"Engineering new features...\")\n",
        "\n",
        "def categorize_procedure(drg_desc):\n",
        "    drg_desc = drg_desc.lower()\n",
        "    if 'transplant' in drg_desc or 'ecmo' in drg_desc or 'tracheostomy' in drg_desc:\n",
        "        return 'Major Surgery/Intensive Care'\n",
        "    elif 'cardiac' in drg_desc or 'heart' in drg_desc or 'valve' in drg_desc:\n",
        "        return 'Cardiology'\n",
        "    elif 'joint' in drg_desc or 'spinal fusion' in drg_desc or 'back & neck' in drg_desc:\n",
        "        return 'Orthopedics'\n",
        "    elif 'nervous system' in drg_desc or 'craniotomy' in drg_desc:\n",
        "        return 'Neurology'\n",
        "    elif 'pulmonary' in drg_desc or 'respiratory' in drg_desc:\n",
        "        return 'Pulmonary'\n",
        "    elif 'septicemia' in drg_desc:\n",
        "        return 'Infections'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df['procedure_category'] = df['drg_desc'].apply(categorize_procedure)\n",
        "\n",
        "# Apply the log transform to the target variable\n",
        "df['avg_total_payment_log'] = np.log1p(df['avg_total_payment'])\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 3. Final Model Preparation\n",
        "# =============================================================================\n",
        "print(\"Preparing data for the final model...\")\n",
        "\n",
        "# Define final features and target\n",
        "features = ['state_abbr', 'ruca_code', 'total_discharges', 'procedure_category']\n",
        "target = 'avg_total_payment_log'\n",
        "\n",
        "X = df[features]\n",
        "y = df[target]\n",
        "\n",
        "# We don't need a train-test split here, as we train the final pipeline on all available data.\n",
        "# This is a common practice for creating a production model.\n",
        "\n",
        "# Define the preprocessing steps\n",
        "categorical_features = ['state_abbr', 'procedure_category']\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('cat', OneHotEncoder(handle_unknown='ignore', drop='first'), categorical_features)\n",
        "    ],\n",
        "    remainder='passthrough'\n",
        ")\n",
        "\n",
        "# Use the best hyperparameters you found during tuning\n",
        "best_params = {\n",
        "    'num_leaves': 31,\n",
        "    'n_estimators': 300,\n",
        "    'min_child_samples': 30,\n",
        "    'max_depth': 7,\n",
        "    'learning_rate': 0.1,\n",
        "    'random_state': 42\n",
        "}\n",
        "\n",
        "# =============================================================================\n",
        "# 4. Build and Train the Final Pipeline\n",
        "# =============================================================================\n",
        "\n",
        "# Create the full pipeline object\n",
        "final_pipeline_v2 = Pipeline(steps=[\n",
        "    ('preprocessor', preprocessor),\n",
        "    ('regressor', LGBMRegressor(**best_params))\n",
        "])\n",
        "\n",
        "print(\"Training the final V2 pipeline on all data...\")\n",
        "# Train the pipeline on the entire dataset\n",
        "final_pipeline_v2.fit(X, y)\n",
        "print(\"Training complete.\")\n",
        "\n",
        "\n",
        "# =============================================================================\n",
        "# 5. Save the Final Pipeline\n",
        "# This is your final, production-ready model artifact for Phase 2.\n",
        "# =============================================================================\n",
        "joblib.dump(final_pipeline_v2, 'insurance_pipeline_v2.joblib')\n",
        "print(\"\\n✅ V2 pipeline saved successfully as 'insurance_pipeline_v2.joblib'!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1HbM3dV2JDlM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}